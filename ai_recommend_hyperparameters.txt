LLM을 활용하여 시계열 모델의 하이퍼파라미터를 최적화하고 성능을 비교합니다.

모델 하이퍼파라미터 최적화
최적화할 모델 선택

lstm
현재 모델 파라미터
{
"n_steps":24
"lstm_units":[
0:32
]
"epochs":30
}

하이퍼파라미터 추천이 완료되었습니다!


AI 추천 하이퍼파라미터
추천 근거 확인

lstm_units: 현재 LSTM 유닛 수가 32개인데, 데이터의 복잡성을 고려하여 64개와 32개의 레이어를 쌓아 모델의 표현력을 높입니다. 레이어 수를 늘려 더 복잡한 패턴을 학습할 수 있도록 합니다.

n_steps: 현재 n_steps가 24인데, 데이터의 자기 상관성을 고려하여 48로 늘립니다. 더 긴 과거 데이터를 활용하여 미래 값을 예측함으로써 예측 정확도를 높일 수 있습니다.

epochs: 현재 epochs가 30인데, 모델이 충분히 학습될 수 있도록 100으로 늘립니다. 다만, 과적합을 방지하기 위해 early stopping 기법을 함께 사용하는 것이 좋습니다.

batch_size: 32로 설정하여 학습 속도와 안정성을 확보합니다. 배치 크기가 너무 크면 메모리 부족 문제가 발생할 수 있고, 너무 작으면 학습 속도가 느려질 수 있습니다.

dropout: 0.2의 dropout 레이어를 추가하여 과적합을 방지합니다. dropout은 학습 과정에서 일부 뉴런을 무작위로 비활성화하여 모델이 특정 뉴런에 과도하게 의존하는 것을 막습니다.

learning_rate: 0.001의 학습률을 사용하여 모델이 최적의 가중치를 찾도록 합니다. 학습률이 너무 크면 모델이 발산할 수 있고, 너무 작으면 학습 속도가 느려질 수 있습니다.

예상 개선 효과
RMSE: RMSE를 10% 이상 감소시킬 것으로 예상됩니다. 더 많은 LSTM 유닛과 긴 n_steps를 통해 모델이 데이터의 복잡한 패턴을 더 잘 학습할 수 있기 때문입니다.

R^2: R^2 점수를 5% 이상 향상시킬 것으로 예상됩니다. 모델의 예측 능력이 향상되어 데이터의 분산을 더 잘 설명할 수 있기 때문입니다.

추천 파라미터 조정
AI가 추천한 파라미터를 필요에 따라 수정할 수 있습니다.

lstm_units

[64,32]
n_steps

48


epochs

100


batch_size

32


dropout

0.2000


learning_rate

0.0010

