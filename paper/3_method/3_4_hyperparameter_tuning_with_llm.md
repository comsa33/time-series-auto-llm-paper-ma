제 4 절 LLM 기반 모델링 자동화 및 프롬프트 설계
(1) LLM 선택 및 실행 방식
시계열 예측 모델의 하이퍼파라미터 튜닝 자동화를 위해 Gemma3:27B 모델을 활용하였다. 이 모델은 2025년 기준 최신 대규모 언어 모델 중 하나로, 복잡한 시계열 분석 문제에 대한 추론 능력과 도메인 지식을 갖추고 있어 하이퍼파라미터 튜닝 조언에 적합하다고 판단하였다.
실행 방식은 Ollama를 통해 Gemma3:27B 모델에 질의하는 형태로 구현하였다. Python 환경에서 OpenAI 호환 API 클라이언트 라이브러리를 사용하여 프롬프트를 전달하고 응답을 받는 방식으로, 애플리케이션 내에서 LLM 쿼리를 프로그래밍 방식으로 생성하고 처리하였다.
LLM 호출 시에는 응답의 일관성과 다양성 간의 균형을 위해 온도(temperature) 파라미터를 0.2로 설정하였다. 이는 반복 실험에서 안정적인 결과를 얻으면서도, 너무 결정론적이지 않은 응답을 생성하기 위한 값이다. 프롬프트와 응답을 포함한 최대 토큰 수는 로컬 실행 환경에서는 제한이 없으나, 응답의 완결성과 처리 시간을 고려하여 실질적으로 약 2,000 토큰 내외로 유지되도록 설계하였다.
LLM 서비스는 별도의 서버에서 실행되어 REST API 방식으로 통신하였으며, 이를 통해 애플리케이션과 LLM 서비스 간의 분리가 가능하여 각 구성 요소의 독립적인 확장과 유지보수가 가능하도록 하였다. 또한 로컬 서버 환경에서 실행함으로써 외부 API 의존성을 줄이고 비용 효율성을 높였다.

(2) 프롬프트 설계 및 예시
LLM에게 효과적으로 정보를 전달하고 구조화된 응답을 얻기 위해 체계적인 프롬프트 엔지니어링을 수행하였다. 프롬프트에는 현재까지의 모델 구성, 학습 데이터 특성, 성능 평가 결과 등을 JSON 형식으로 포함하여 LLM이 정확한 정보를 바탕으로 판단할 수 있도록 하였다.
각 모델의 하이퍼파라미터 튜닝을 위한 프롬프트는 다음과 같은 구조로 설계하였다:
# 시계열 모델 하이퍼파라미터 추천 요청
당신은 시계열 데이터 분석과 모델링 전문가입니다. 아래 제공된 데이터 정보와 모델 결과를 바탕으로 해당 모델의 성능을 향상시킬 수 있는 하이퍼파라미터를 추천해주세요.
## 데이터 정보
```json
{data_info}
## 현재 모델 정보
{model_info}
## 현재 모델 성능
{performance_metrics}
응답은 반드시 다음 JSON 형식으로 작성해주세요:
```json
{
  "model_type": "모델 유형(arima, prophet, exp_smoothing, lstm 중 하나)",
  "recommended_parameters": {
    "param1": value1,
    "param2": value2
  },
  "rationale": {
    "param1": "param1 추천 근거(한국어로)",
    "param2": "param2 추천 근거(한국어로)"
  },
  "expected_improvement": {
    "metric1": "예상 개선 효과 1(한국어로)",
    "metric2": "예상 개선 효과 2(한국어로)"
  }
}
```

이와 같은 프롬프트에 실제 데이터 정보와 모델 성능 정보를 JSON 형식으로 삽입하여 동적으로 LLM에 전달하였다. 데이터 정보에는 시계열의 기본 통계량, 추세 및 계절성 특성, 정상성 여부 등이 포함되었으며, 모델 정보에는 현재 하이퍼파라미터 설정과 모델 구조가 상세히 기술되었다. 성능 지표에는 RMSE, MAE, R² 등 모델의 예측 성능을 평가하는 다양한 메트릭이 포함되었다.
응답을 JSON 형식으로 지정함으로써 LLM의 출력을 프로그램에서 직접 파싱하여 사용할 수 있도록 하였다. 이는 자동화된 파이프라인에서 사람의 개입 없이 LLM의 제안을 바로 모델 학습에 반영할 수 있게 한다는 점에서 중요하다.
실제 프롬프트 예시와 LLM 응답의 일부는 다음과 같다:

1)	프롬프트 예시 (LSTM 모델):
```json
{
  "model_type": "lstm",
  "data_info": {
    "target_variable": "PM25",
    "data_length": 742,
    "time_period": "hourly",
    "seasonality_period": 24,
    "statistics": {
      "min": 2.08,
      "max": 80.36,
      "mean": 27.20,
      "std": 14.56
    },
    "stationarity": {
      "is_stationary": false,
      "p_value": 0.24
    }
  },
  "model_info": {
    "n_steps": 24,
    "lstm_units": 50,
    "dropout_rate": 0.2,
    "epochs": 80
  },
  "performance_metrics": {
    "MSE": 22.09,
    "RMSE": 4.70,
    "MAE": 3.25,
    "R^2": 0.82,
    "MAPE": 23.41
  }
}

2)	LLM 응답 예시:
```json
{
  "model_type": "lstm",
  "recommended_parameters": {
    "lstm_units": 100,
    "n_steps": 24,
    "dropout_rate": 0.2,
    "epochs": 80
  },
  "rationale": {
    "lstm_units": "현재 데이터의 복잡한 패턴을 더 잘 포착하기 위해 LSTM 유닛 수를 50에서 100으로 증가시키는 것이 좋습니다. PM2.5 데이터는 다양한 환경 요인의 영향을 받아 복잡한 시간적 의존성을 가지므로, 모델의 표현력을 높이는 것이 중요합니다.",
    "n_steps": "현재 24시간으로 설정된 입력 시퀀스 길이는 데이터의 일별 주기성을 잘 포착하고 있어 유지하는 것이 좋습니다. 더 긴 시퀀스로 변경하면 과거 데이터에 과도하게 의존할 수 있습니다.",
    "dropout_rate": "현재의 0.2 드롭아웃 비율은 과적합과 일반화 사이의 균형점으로 적절합니다. 데이터 크기를 고려할 때 이 값을 유지하는 것이 안정적인 학습에 도움이 됩니다.",
    "epochs": "80 에폭은 충분한 학습을 제공하면서도 조기 종료와 함께 사용하면 과적합을 방지할 수 있으므로 적절합니다."
  },
  "expected_improvement": {
    "RMSE": "유닛 수 증가로 RMSE가 4.70에서 약 3.0~3.5 사이로 개선될 것으로 예상됩니다(약 25-35% 감소).",
    "R^2": "모델의 표현력 향상으로 R² 값이 0.82에서 0.88~0.92 수준으로 개선될 것으로 예상됩니다.",
    "MAPE": "시간적 패턴을 더 잘 포착하여 MAPE가 23.41%에서 약 15-18% 수준으로 개선될 것으로 예상됩니다."
  }
}
```

이러한 JSON 형식의 응답은 프로그램에서 쉽게 파싱할 수 있으며, 자동화된 파이프라인에서 즉시 모델 재학습에 활용할 수 있다. 또한 각 파라미터 변경에 대한 근거와 예상 개선 효과를 포함함으로써, 단순한 파라미터 제안을 넘어 교육적 가치도 제공한다.

(3) LLM 기반 하이퍼파라미터 튜닝 피드백 루프
LLM을 활용한 하이퍼파라미터 튜닝은 반복적인 피드백 루프 형태로 구현되었다. 이 과정은 [그림2]의 흐름도와 같이 진행된다. 초기에는 도메인 지식과 기존 문헌을 참고하여 선정한 기본 하이퍼파라미터로 모델을 학습시킨 후, 그 결과를 LLM에게 제공하여 개선 방안을 질의한다. LLM은 데이터와 모델 특성을 분석하여 하이퍼파라미터 조정 제안을 반환하며, 이를 적용하여 모델을 재학습한다. 재학습 후 성능을 평가하여 개선 여부를 확인하고, 필요에 따라 이 과정을 반복한다.
본 연구에서는 각 모델에 대해 최대 3회의 LLM 피드백 루프를 적용하였으며, 각 단계마다 성능 개선이 있는 경우에만 변경사항을 유지하였다. 예를 들어, LSTM 모델의 경우 초기에는 은닉노드 50개로 설정했으나, LLM이 “패턴이 복잡하므로 LSTM 유닛 수를 늘려보라”는 제안에 따라 은닉노드를 100개로 증가시켰고, 실제로 RMSE가 4.70에서 3.24로 감소하는 성능 향상을 확인하였다. 또한 “과적합 방지를 위해 조기 종료를 적용하라”는 제안에 따라 EarlyStopping 콜백을 추가하여 더 안정적인 학습을 도모하였다.
반면, Transformer 모델에서는 LLM이 "데이터가 적으면 과적합되기 쉬우니 head 수를 줄이라"고 제안하여 4-head를 2-head로 변경해보았으나, 오히려 성능이 저하되어 다시 원래 설정으로 되돌렸다. 이는 모든 LLM 제안이 항상 개선으로 이어지지는 않음을 보여주며, 실험적 검증의 중요성을 강조한다.
이러한 LLM 기반 피드백 루프는 다음과 같은 장점을 제공한다:
1.	효율성: 전통적인 그리드 탐색이나 랜덤 탐색에 비해 훨씬 적은 시도로 효과적인 하이퍼파라미터 조합을 발견할 수 있다.
2.	도메인 지식 활용: LLM에 내재된 시계열 분석 및 모델링 지식을 활용하여 맥락에 맞는 조정 제안을 얻을 수 있다.
3.	설명 가능성: 단순한 파라미터 값 제안을 넘어 그 근거를 함께 제공하므로, 사용자는 변경의 이유를 이해할 수 있다.
4.	자동화: 사람의 개입 없이도 모델 튜닝 과정이 자동화될 수 있어, 데이터 사이언티스트의 시간과 노력을 절감한다.
특히 LSTM 모델의 경우, 이러한 LLM 기반 튜닝을 통해 MSE가 22.09에서 7.63으로, RMSE가 4.70에서 2.76으로 대폭 감소하였다. 이는 약 65%의 MSE 감소와 41%의 RMSE 감소를 의미하며, 하이퍼파라미터 튜닝의 효과를 명확히 보여준다.

(4) 재현성과 신뢰성 확보 방안
LLM을 활용한 모델링 과정의 재현성(reproducibility)과 신뢰성을 확보하기 위해 다음과 같은 방안을 적용하였다.
첫째, 모든 실험에서 난수 시드(random seed)를 42로 고정하여 모델 가중치 초기화, 배치 샘플링 등의 무작위 요소가 실험 간 결과 변동을 일으키지 않도록 하였다. 이는 Python의 random 모듈, NumPy, TensorFlow 및 PyTorch의 시드를 모두 동일하게 설정하여 구현하였다.
둘째, LLM 응답의 확률적 특성으로 인한 변동성을 줄이기 위해 프롬프트를 최대한 구체적이고 구조화된 형태로 작성하였으며, 온도(temperature) 파라미터를 0.2로 낮게 설정하여 응답의 일관성을 높였다. 또한 JSON 형식의 출력을 요청함으로써 응답의 구조적 일관성을 확보하였다.
셋째, 각 피드백 루프 단계에서 다음과 같은 정보를 포함한 실험 로그를 자동으로 기록하였다:
-	원본 프롬프트 내용
-	LLM의 응답 전문 (JSON 형식)
-	적용된 하이퍼파라미터 변경 사항
-	변경 전후 모델 성능 지표
-	예측 결과 그래프 및 잔차 분석
이 로그는 향후 동일한 실험을 재현하거나, LLM 응답의 품질을 평가하는 데 활용될 수 있다.
넷째, LLM의 제안을 자동으로 적용하기 전에 유효성 검증 단계를 거쳤다. 예를 들어, 하이퍼파라미터 값이 합리적인 범위를 벗어나는 경우(예: 비현실적으로 큰 은닉층 크기) 이를 필터링하여 모델 학습이 실패하지 않도록 하였다. 또한 JSON 파싱 오류나 잘못된 파라미터명과 같은 기술적 문제를 방지하기 위한 예외 처리를 구현하였다.
다섯째, A/B 테스트 방식으로 LLM 제안의 효과를 검증하였다. 즉, 기존 모델과 LLM 제안을 적용한 모델을 동일 조건에서 비교하여 성능 향상이 통계적으로 유의미한 경우에만 변경을 채택하였다. 이를 통해 단순한 무작위 변동으로 인한 성능 차이를 배제하고 실질적인 개선만을 수용하였다.
이러한 방안을 통해 LLM 기반 자동화 모델링 과정의 재현성과 신뢰성을 확보함으로써, 연구 결과의 타당성을 높이고 실제 응용 환경에서도 안정적으로 활용될 수 있는 토대를 마련하였다.

(5) 자동 분석 보고서 생성
본 연구에서 제안하는 시스템의 또 다른 핵심 구성요소는 LLM을 활용한 분석 보고서 자동 생성 모듈이다. 이 모듈은 모든 모델의 예측 결과와 성능 지표를 종합하여, 사람이 작성한 것과 유사한 품질의 분석 문서를 자동으로 생성한다.
자동 보고서 생성 프로세스는 크게 다음과 같은 단계로 구성된다:
1.	데이터 수집 및 구조화: 모든 모델의 예측 결과, 성능 지표(MSE, RMSE, MAE, R², MAPE), 그래프 데이터 등을 JSON 형식으로 구조화한다.
2.	LLM 프롬프트 구성: 수집된 데이터를 바탕으로 분석 보고서 생성을 위한 프롬프트를 구성한다. 이 프롬프트에는 보고서에 포함되어야 할 주요 섹션과 내용에 대한 지침이 포함된다.
3.	LLM 추론 실행: 구성된 프롬프트를 LLM에 전달하여 보고서 생성을 요청한다. 이때 Gemma3:27B 모델을 활용하며, 상세하고 구조화된 보고서를 생성할 수 있도록 충분한 토큰 길이를 할당한다.
4.	마크다운 형식 보고서 생성: LLM은 전달받은 데이터를 분석하여 마크다운 형식의 구조화된 보고서를 생성한다. 이 보고서는 표, 강조 표시, 제목 등의 서식을 포함하여 가독성을 높인다.
5.	웹 인터페이스 표시 및 다운로드: 생성된 보고서는 스트림릿 웹 인터페이스에 바로 렌더링되어 사용자에게 표시되며, 필요에 따라 마크다운 파일로 다운로드할 수 있는 기능을 제공한다.
LLM이 생성한 보고서는 단순히 결과를 나열하는 수준을 넘어, 전문가가 작성한 것과 유사한 수준의 분석과 인사이트를 제공한다. 예를 들어, 모델 간 성능 비교, 최적 모델 추천 및 그 근거, 예측 결과의 신뢰도 평가, 향후 개선 방향 제안 등을 포함한다. 특히 LSTM 모델이 여러 메트릭에서 우수한 성능을 보이며, 이것이 PM2.5 농도의 시간적 패턴을 효과적으로 포착했기 때문이라는 분석을 제공하는 등, 데이터와 모델의 특성을 깊이 있게 이해하는 모습을 보여준다.
이러한 자동 보고서 생성 기능은 데이터 분석 워크플로우에서 여러 이점을 제공한다:
1.	효율성: 데이터 사이언티스트가 수동으로 보고서를 작성하는 시간을 절약한다.
2.	일관성: 보고서의 품질과 형식이 항상 일정한 수준으로 유지된다.
3.	접근성: 전문가 수준의 분석을 비전문가도 쉽게 이해할 수 있는 형태로 제공한다.
4.	확장성: 다양한 데이터셋과 모델에 대해 동일한 방식으로 보고서를 생성할 수 있다.
이처럼 자동 분석 보고서 생성 모듈은 예측 모델링 과정을 완전 자동화하는 마지막 퍼즐 조각으로서, 데이터 사이언티스트의 역할 중 상당 부분을 생성형 AI가 대체할 수 있음을 보여주는 중요한 사례라 할 수 있다.
