3.5 자동 분석 보고 생성 및 구현 환경
모델 훈련과 평가가 완료된 후, 본 연구에서는 LLM을 이용한 결과 보고서 자동 작성을 수행하였다. 보고서 생성에는 앞 단계와 동일한 Gemma3:27B 모델을 활용하였으며, 프롬프트로는 최종 선정된 모델과 그 성능, 그리고 전체 모델 비교 결과 등을 서술하도록 요구했다. 예를 들어 LLM에게 다음과 같은 지시를 하였다:
당신은 데이터 사이언스 분야의 연구원입니다. 다음은 다양한 모델로 PM2.5 농도를 예측한 결과입니다:

- Exponential Smoothing: RMSE=251.16, R^2=-548.02, MAPE=1619.78
- ARIMA: RMSE=13.41, R^2=-0.56, MAPE=100.22
- Prophet: RMSE=31.86, R^2=-7.84, MAPE=223.82
- LSTM: RMSE=4.29, R^2=0.84, MAPE=19.63
- Transformer: RMSE=5.47, R^2=0.74, MAPE=26.33

위 성능 지표를 비교 분석하고, 어느 모델이 가장 우수한지 이유와 함께 설명하세요. 또한 각 모델의 장단점을 데이터 특성과 연결지어 서술하고, 예측 결과를 개선하기 위한 제안도 제시하세요. 한국어로 Markdown 형식으로 작성하세요.
LLM은 이에 대해 매우 상세한 보고서 초안을 작성해 주었다. 연구자는 이 초안을 검토하여 부정확한 부분을 수정하거나, 추가로 강조할 내용을 보완하였다. 최종 보고서는 4장에서 제시하겠지만, 여기에는 모델별 성능 비교 표, 최적 모델 선정 및 이유, 모델 장단점 분석 표, 향후 개선 방안 등이 모두 포함되었다. 놀랍게도 LLM이 생성한 초안은 대부분 정확했고, 논문 스타일에 맞추기 위해 일부 문체와 형식을 손보는 정도의 최소한 편집만 필요했다. 이를 통해 확인할 수 있듯, LLM은 단순 튜닝뿐 아니라 결과 해석 및 서술 측면에서도 유용한 도구임을 알 수 있었다.
시스템의 구현 환경을 요약하면 다음과 같다. 개발은 Python 3.10 환경에서 진행되었고, 주요 패키지로 statsmodels, pmdarima, fbprophet, scikit-learn, TensorFlow/Keras, PyTorch 등을 사용하였다. LLM 모델(Gemma3:27B)은 24GB VRAM을 갖춘 GPU에서 실행하였으며, transformers 라이브러리를 통해 로드하였다. Streamlit 앱은 로컬 호스트에서 구동하여 실험을 인터랙티브하게 제어하는 데 활용되었다. 전체 코드와 보고서 생성 프로세스는 재현 가능성을 위해 깃허브 저장소에 공개해 두었다.
이상의 연구 방법을 통해, 모델 학습과 평가, LLM과의 상호작용, 보고서 생성까지 모두 자동화된 시계열 예측 워크플로우를 구축하였다. 다음 4장에서는 이러한 방법을 적용한 실험 결과를 상세히 기술한다.
