3.3 모델 구현 및 구성

본 연구에서는 3.2절의 데이터를 이용하여 앞서 언급한 다섯 가지 시계열 예측 모델을 모두 구현하고 비교하였다. 각 모델의 구체적인 구현 방식과 초기 하이퍼파라미터 설정은 다음과 같다.
지수평활법 모델: statsmodels 라이브러리의 Holt-Winters 구현을 사용하였다. 데이터의 24시간 주기를 반영하기 위해 **계절 기간(seasonal period)**을 24로 설정하고, 계절 패턴과 추세를 모두 고려하는 Additive Triple Exponential Smoothing을 적용하였다. 초기 smoothing 계수(alpha, beta, gamma)는 라이브러리의 자동 최적화 기능을 사용하여 추정하였으며, 예측은 테스트 구간(149시간)에 대해 direct forecasting으로 한 번에 149시간치를 예측하도록 하였다.


ARIMA 모델: 데이터의 ACF/PACF를 참고하여 초기 모형 차수를 선정하고 pmdarima의 auto_arima 함수를 활용하여 구체적인 (p,d,q) 및 (P,D,Q,m)을 탐색하였다. 일중 계절성을 반영하기 위해 **계절 차분(24시간 주기)**을 1회 적용하는 SARIMA 모델을 고려하였다. 초기 값으로 (p,d,q)=(2,0,2), (P,D,Q)=(1,1,1) 등의 조합을 시도하였고, AIC 기준으로 가장 적합한 모델을 선택하였다. 최종적으로는 ARIMA(1,0,1) (비계절) 모델이 선택되었는데, 이는 데이터의 하루 주기 변동이 강하지만 auto_arima가 짧은 기간 내에서는 계절성을 배제한 모형을 선호했기 때문으로 판단된다. (계절 모형의 경우 파라미터가 늘어나 작은 데이터셋에서는 과적합 우려가 있음)


Prophet 모델: fbprophet 라이브러리를 사용하여 구현하였다. Prophet의 기본 설정을 적용하되, 데이터 기간이 1개월로 비교적 짧으므로 연간/월간 계절성은 비활성화하고 일별(日別) 계절성을 수동으로 추가하였다. 또한 변화점(change point) 탐지 민감도를 약간 높이기 위해 changepoint_prior_scale을 기본 0.05에서 0.1로 증가시켜 갑작스런 추세 변화에 대응하고자 했다. 모델 학습에는 기본 Bayesian sampling 방법을 사용하여 1000번의 샘플링(iterations)으로 사후분포를 추정하였으며, 예측은 테스트 구간에 대해 수행하였다.


LSTM 모델: 파이썬의 TensorFlow-Keras를 활용하여 LSTM 신경망을 구성하였다. 입력 시퀀스 길이는 24시간 (과거 24개의 시점의 농도)으로 설정하여 하루 주기의 패턴을 학습할 수 있도록 하였다. 네트워크 구조는 **LSTM 레이어 1개 (은닉 노드 100개)**와 뒤에 완전연결(Dense) 출력 레이어로 구성된 단층 LSTM으로 시작하였고, 과적합을 막기 위해 LSTM 층에 드롭아웃(dropout) 0.2를 적용하였다. 손실 함수는 회귀 문제이므로 MSE를 사용하고, Adam 옵티마이저로 학습률 0.001로 80 epochs 학습하였다. 초기에는 이 기본 구조로 학습하고, 이후 LLM의 조언에 따라 레이어 수나 노드 수 등을 변경하기도 했다. 예를 들어 1차 실험 후 LLM이 **“패턴이 복잡하므로 LSTM 유닛 수를 늘려보라”**는 제안을 하여 은닉 노드를 100개로 늘렸고, **“너무 과적합 될 수 있으므로 조기 종료를 사용하라”**는 조언에 따라 조기 종료(EarlyStopping) 기법을 도입하였다. 최종적으로는 은닉 노드 100개, epoch 80 (조기종료 모니터링)으로 훈련한 모델이 최상의 검증 성능을 보여 이를 채택하였다.


Transformer 모델: PyTorch 기반으로 시계열용 Transformer 모델을 구성하였다. 입력 역시 최근 24시간 시계열로 설정하고, 인코더-디코더 구조 대신 간소화된 시계열 예측 아키텍처를 사용하였다. Transformer 인코더 블록을 2개 쌓고, 멀티헤드 어텐션은 각 블록당 4-head, 모델 차원은 64로 두었다. 포지셔널 인코딩을 추가하여 순서 정보를 주입하고, 마지막에 선형 레이어로 한 시간 단위 출력을 얻도록 설계하였다. 학습 파라미터로 epoch 50, learning rate 0.001, Adam 옵티마이저를 사용하였다. 이 모델 역시 초기 설정 후 LLM의 튜닝 제안을 받았다. LLM은 **“Transformer는 데이터가 적으면 과적합되기 쉬우니 head 수를 줄이라”**고 조언하여 4-head를 2-head로 줄여보았으나 성능이 오히려 저하되어 다시 4-head로 유지하였다. 대신 정규화와 드롭아웃을 추가하여 일반화를 도모하였다. 최종 Transformer 모델은 인코더 2층, 각층 4-head, 드롭아웃 0.1 적용으로 확정하였다.


위와 같이 각 모델은 기본적으로 표준 구현을 따르되, 데이터의 특성(24시간 계절성, 데이터량 등)에 맞춰 하이퍼파라미터를 설정하였다. 하지만 이러한 초기 설정은 도메인 지식과 일부 시행착오에 근거한 것으로, 반드시 최적이라고 할 수 없다. 따라서 다음 절에서 설명할 LLM을 통한 자동 튜닝 절차를 통해 추가적인 성능 향상을 도모하였다.
(모델별 주요 하이퍼파라미터 요약) 본 연구에서 최종적으로 사용된 모델들의 핵심 하이퍼파라미터를 표 3.1에 정리한다. 이들 값은 LLM의 추천과 실험적 검증을 거쳐 확정된 것이다.

표 3.1: 모델별 하이퍼파라미터 설정 (최종값)
| 모델            | 주요 하이퍼파라미터 (최종 설정값)                                                                 |
|-----------------|--------------------------------------------------------------------------------------------------|
| Exponential Smoothing | 계절 주기=24시간, 추세=Additive, 계절성=Additive, 초기 α/β/γ 자동추정 (Holt-Winters)                     |
| ARIMA           | 모형 차수 (p,d,q)=(1,0,1), 계절 차수 (P,D,Q,m)=(0,0,0,0) (계절성 미포함)                              |
| Prophet         | 일별 계절성 포함 (주별/연별 제외), changepoint_prior_scale=0.1, 기타 기본값                              |
| LSTM            | 입력 시퀀스 길이=24, LSTM 은닉노드=100, LSTM층=1, Dropout=0.2, 학습률=0.001, Epoch=80 (조기종료)         |
| Transformer     | 인코더 층=2, 모델 차원=64, 어텐션 head=4, Dropout=0.1, 학습률=0.001, Epoch=50                          |
