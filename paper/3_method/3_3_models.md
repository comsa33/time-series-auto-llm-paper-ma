제 3 절 시계열 예측 모델 구성 및 학습
(1) 모델 구성 요소
본 연구에서는 서울시 대기질 데이터의 시계열 예측을 위해 통계 기반 모델과 기계학습 기반 모델을 포괄하는 5가지 주요 접근법을 구현하였다. 각 모델의 구체적인 구성 요소는 다음과 같다.
지수평활법(Exponential Smoothing) 모델: statsmodels 라이브러리의 Holt-Winters 구현을 사용하여 데이터의 추세와 계절성을 모두 고려하는 삼중 지수평활법을 적용하였다. 시간별 데이터의 24시간 주기성을 반영하기 위해 계절 기간(seasonal period)을 24로 설정하고, 추세 및 계절성 구성요소는 가법적(Additive) 모델로 구현하였다. smoothing 계수(α, β, γ)는 초기에 라이브러리의 자동 최적화 기능을 통해 최적 값을 추정하였다.
ARIMA/SARIMA 모델: pmdarima 라이브러리의 auto_arima 함수를 활용하여 최적의 모형 차수를 탐색하였다. 초기에는 데이터의 ACF/PACF 분석을 통해 (p,d,q)=(2,0,2), (P,D,Q)=(1,1,1,24)와 같은 계절성을 포함한 SARIMA 모델을 고려하였으나, 데이터의 기간이 제한적인 특성으로 인해 최종적으로는 ARIMA(1,0,1)이 선택되었다. 이는 작은 데이터셋에서 과적합을 방지하기 위함이며, AIC 기준으로 최적 모델을 선정하였다.
Prophet 모델: Facebook에서 개발한 fbprophet 라이브러리를 사용하여 시계열 분해 기반의 예측 모델을 구현하였다. 1개월이라는 짧은 데이터 기간을 고려하여 연간/월간 계절성은 비활성화하고 일별(日別) 계절성만 활성화하였다. 변화점(changepoint) 탐지 민감도를 제어하는 changepoint_prior_scale 파라미터는 기본값 0.05에서 0.1로 상향 조정하여 갑작스러운 농도 변화를 더 잘 포착하도록 하였다.
LSTM(Long Short-Term Memory) 모델: TensorFlow-Keras를 활용하여 순환 신경망 기반 모델을 구축하였다. 입력 시퀀스 길이는 24시간으로 설정하여 하루 주기의 패턴을 학습할 수 있도록 하였다. 네트워크 구조는 LSTM 레이어 1개(은닉 노드 100개)와 완전연결(Dense) 출력 레이어로 구성하였으며, 과적합 방지를 위해 드롭아웃(dropout) 0.2를 적용하였다. 손실 함수는 평균제곱오차(MSE)를 사용하고, Adam 옵티마이저로 학습률 0.001, 배치 크기 32로 학습을 수행하였다. 또한 조기 종료(Early Stopping) 기법을 적용하여 검증 손실이 개선되지 않을 경우 학습을 중단하도록 하였다.
Transformer 모델: PyTorch 기반으로 시계열 데이터에 특화된 Transformer 모델을 구현하였다. 인코더-디코더 구조 대신 간소화된 인코더 중심의 아키텍처를 사용하였으며, 인코더 블록 2개, 각 블록당 멀티헤드 어텐션 4-head, 모델 차원 64로 설계하였다. 포지셔널 인코딩을 추가하여 시간적 순서 정보를 보존하였고, 최종 출력층에서는 선형 변환을 통해 다음 시간의 예측값을 생성하도록 하였다. 학습 안정성을 위해 드롭아웃(0.1)과 레이어 정규화를 적용하였다.

(2) 학습 하이퍼파라미터 설정
각 모델의 학습에 사용된 주요 하이퍼파라미터를 <표 1>과 같이 정리한다. 이 값들은 초기 설정에서 출발하여 LLM의 제안과 실험적 검증을 거쳐 최종 결정된 것이다.
<표 1> 모델별 하이퍼파라미터 설정 (최종값)
모델	주요 하이퍼파라미터 (최종 설정값)
지수평활법	계절 주기=24시간, 추세=Additive, 계절성=Additive, 초기 α/β/γ 자동추정 (Holt-Winters)
ARIMA	모형 차수 (p,d,q)=(1,0,1), 계절 차수 (P,D,Q,m)=(0,0,0,0) (계절성 미포함)
Prophet	일별 계절성 포함 (주별/연별 제외), changepoint_prior_scale=0.1, 기타 기본값
LSTM	입력 시퀀스 길이=24, LSTM 은닉노드=100, LSTM층=1, Dropout=0.2, 학습률=0.001, Epoch=80 (조기종료)
Transformer	인코더 층=2, 모델 차원=64, 어텐션 head=4, Dropout=0.1, 학습률=0.001, Epoch=50

LSTM 모델의 경우, 입력 시퀀스 길이를 24시간으로 설정하여 하루 주기의 패턴을 효과적으로 학습할 수 있도록 하였다. 이는 데이터의 일중(日中) 계절성을 고려한 것으로, PM2.5 농도가 시간대별로 뚜렷한 패턴을 보인다는 도메인 지식을 반영한 것이다. 은닉 노드 수는 초기 50개에서 100개로 증가시켰는데, 이는 LLM의 제안과 실험 결과에 따른 것으로 모델의 표현력을 향상시키는 효과가 있었다.
Transformer 모델에서는 멀티헤드 어텐션의 head 수를 4개로 설정하였다. LLM은 데이터 양이 적을 경우 head 수를 줄이는 것이 과적합 방지에 도움이 될 수 있다고 제안하였으나, 실험 결과 4-head가 2-head보다 성능이 우수하여 유지하였다. 대신 드롭아웃을 0.1로 적용하여 과적합을 방지하였다.
모든 딥러닝 모델(LSTM, Transformer)에는 Adam 옵티마이저를 사용하였으며, 학습률은 0.001로 설정하였다. 이는 모델이 너무 빠르게 수렴하여 지역 최적해에 빠지거나, 너무 느리게 학습되어 충분한 성능에 도달하지 못하는 상황을 방지하기 위한 균형점으로 선택되었다. 또한 평균제곱오차(MSE)를 손실 함수로 사용하여 예측값과 실제값 간의 차이를 최소화하는 방향으로 학습이 이루어지도록 하였다.
