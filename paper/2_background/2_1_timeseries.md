2.1 시계열 예측 모델 개요

시계열 예측을 위해 본 연구에서 다룬 대표적인 모델들은 통계적 방법과 딥러닝 방법으로 나눌 수 있다. **지수평활법(Exponential Smoothing)**은 과거 데이터에 지수적으로 감쇠하는 가중치를 부여하여 미래를 예측하는 기법으로, **Brown(1956)**과 Holt(1957)[2] 등에 의해 체계화되었고 이후 **Winters(1960)**[2]에 의해 계절성까지 포함한 삼중 지수평활법으로 확장되었다 . 지수평활법은 구현이 간단하고 계산이 빠르며 비교적 짧은 기간의 추세와 계절 패턴을 포착하는 데 사용된다. 그러나 뚜렷한 추세나 복잡한 계절 변동이 있는 경우에는 정확도가 떨어질 수 있고, 예측 결과가 장기적인 패턴을 충분히 반영하지 못하는 한계가 있다.

ARIMA(Autoregressive Integrated Moving Average) 모델은 자기회귀(ar)와 이동평균(ma), 차분(d)의 세 요소를 조합하여 시계열 데이터를 모형화하는 전통적인 방법이다. ARIMA 모델은 Box와 Jenkins[1]에 의해 1970년 소개된 이래 시계열 분석의 대표적인 기법으로 널리 활용되었다 . Box-Jenkins 방법론은 시계열의 자기상관성을 활용하여 적절한 모형 차수(p, d, q)를 식별하고 모수 추정과 진단을 거쳐 예측을 수행한다. ARIMA는 데이터의 정상성(stationarity) 가정 하에 동작하며, 계절 변동이 있는 경우 계절형 ARIMA(SARIMA)로 확장하여 계절 차분 및 계절 모수를 포함할 수 있다. ARIMA의 장점은 자기상관 구조를 명시적으로 모델링하여 단기 예측에 강점이 있다는 점이며, 단점은 비정상 시계열의 차분 작업, 계절성에 대한 별도 모형화 등이 필요하여 과정이 복잡하고, 모형 식별을 위한 전문 지식이 요구된다는 점이다.

Prophet은 2017년 페이스북(Facebook)에서 공개한 오픈소스 예측 라이브러리로, **Taylor와 Letham(2018)**[3]의 연구에 기반한 가법 모델(additive model) 접근법을 사용한다. Prophet 모델은 시계열을 **장기 추세(trend) + 계절성(seasonality) + 휴일 효과(holidays)**의 합으로 보고 이를 개별적으로 모델링한다. 추세는 선형 또는 로그형 성장으로, 계절성은 주기적인 패턴(예: 연중, 주중, 일중 주기)으로 표현하며, 휴일이나 이벤트의 효과를 추가적인 회귀요소로 포함한다 . Prophet의 가장 큰 장점은 사용 편의성으로, 데이터에 내재된 추세와 계절성을 자동으로 처리하고 이상치에 강건하게 동작하도록 설계되었다. 별도의 정상성 변환이나 복잡한 모수 결정 과정 없이도 기본값으로도 빠르게 양질의 예측을 얻을 수 있다는 점에서 실무 영역에서 인기를 얻고 있다. 다만 Prophet은 복잡한 패턴(예를 들어, 추세 변화가 빈번하거나 불규칙한 패턴)의 예측력에서는 한계가 있을 수 있으며, 충분한 과거 데이터가 없는 경우에는 불확실성이 커지는 단점이 있다.

LSTM(Long Short-Term Memory)은 **순환 신경망(RNN)**의 일종으로, 시계열과 같은 순차 데이터의 장기 의존성(long-term dependency) 문제를 해결하기 위해 고안된 딥러닝 모델이다[4] . RNN은 이전 시점의 출력이나 상태를 다음 시점의 입력으로 사용함으로써 시계열 패턴을 학습하지만, 기울기 소실(vanishing gradient) 문제로 긴 시퀀스의 정보를 전달하기 어려웠다. LSTM은 **셀 상태(cell state)**와 입력게이트, 출력게이트, 망각게이트 등의 구조를 도입하여 불필요한 정보는 잊고(long short-term forget), 필요한 정보는 장기간 기억할 수 있도록 함으로써 RNN의 한계를 극복하였다. 이로 인해 LSTM은 수백 시점에 이르는 긴 시계열도 학습이 가능하며, 시계열 데이터의 복잡한 패턴까지 포착하는 데 뛰어난 성능을 보인다. 다만 LSTM 모델은 많은 수의 파라미터를 가지므로 충분한 훈련 데이터가 필요하고 학습에 시간이 오래 걸릴 수 있다. 또한 모델의 내부 구조가 복잡하여 예측 결과를 해석하기 어렵다는 단점도 있다.

Transformer 모델은 2017년 Vaswani 등에 의해 제안된 어텐션 기반 딥러닝 아키텍처로, seq2seq(sequence-to-sequence) 모델의 새로운 패러다임을 열었다[5] . Transformer의 핵심은 자기어텐션(self-attention) 메커니즘으로, 시퀀스 내 모든 위치의 데이터를 상호 참조하여 관계를 학습한다. 이를 통해 RNN처럼 순차적으로 계산하지 않고 병렬 처리로 학습을 수행할 수 있어, 매우 긴 시퀀스도 효율적으로 학습 가능하고 병렬화로 인한 속도 이점도 얻을 수 있다. Transformer는 원래 자연어 처리의 기계번역 등에서 혁신적인 성능을 보였으며, 이후 다양한 분야로 확산되어 범용 아키텍처로 자리잡았다. 시계열 예측 분야에서도 Transformer 기반 예측 모델들이 등장하여 LSTM 이상의 성능을 보이는 사례들이 보고되고 있다. Transformer의 장점은 멀리 떨어진 시점 간의 상관관계도 학습할 수 있는 강력한 표현력과 학습 효율이며, 단점으로는 많은 층과 헤드(head)를 갖는 복잡한 구조로 인해 모델 학습에 대량의 데이터가 필요하고, 결과를 해석하거나 원인을 추적하기 어려운 점이 있다.

요약하면, 전통 모델(지수평활법, ARIMA, Prophet)은 비교적 구현이 간단하고 명확한 통계적 해석이 가능하나 복잡한 패턴을 충분히 포착하지 못할 수 있다. 반면 딥러닝 모델(LSTM, Transformer)은 비선형적이고 복잡한 패턴 학습에 강력하지만 많은 데이터와 계산자원이 요구되며 모델 해석력 부족 문제가 있다. 이러한 모델들의 특성과 한계를 이해하고 문제에 맞게 선택하거나 결합하는 것이 시계열 예측 성능 향상의 열쇠이다.
