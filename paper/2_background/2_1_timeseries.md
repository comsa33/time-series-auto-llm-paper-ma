제 Ⅱ 장 이론적 배경
제 1 절 시계열 예측 모델 개요
시계열 예측에는 통계적 모형부터 최신 기계학습 기반 모형까지 다양한 접근법이 활용된다. 본 절에서는 대표적인 시계열 예측 기법인 지수평활법(Exponential Smoothing), ARIMA, Prophet, LSTM, Transformer 등의 개념과 배경을 정리한다. 각 모형의 이론적 토대를 제시한 원문 문헌을 기반으로 정확한 정의와 특징을 설명하며, 기존 서술에서 원문과 불일치했던 부분을 교정한다.
지수평활법(Exponential Smoothing). 지수평활법은 과거 관측값에 대한 지수가중치를 사용하여 최신 데이터에 더 큰 가중치를 부여함으로써 미래를 예측하는 방법이다. Brown(1956)이 단순 지수평활법을 처음 제안하여 재고 수요 예측에 활용한 이후, Holt(1957)은 이를 발전시켜 추세(Trend)를 반영할 수 있는 이중 지수평활법을 고안하였다. 이어서 Winters(1960)는 홀트 방법에 계절성(Seasonality) 요인을 추가하여 삼중 지수평활법을 완성하였다. 이처럼 Brown–Holt–Winters로 이어지는 연구를 통해 지수평활법은 수준(level), 추세, 계절 요인을 단계적으로 포함하는 예측 모형으로 정립되었으며, 계산의 단순성과 효용성 때문에 오늘날까지도 Holt-Winters 기법으로 불리며 널리 사용되고 있다. 세 연구자의 원문에서 제시된 지수평활법 공식과 그 이론적 가정들은 본 논문의 기존 기술 내용과 일치하며, 다만 Winters 기법의 경우 “승법형”과 “가법형” 계절 효과 모형이 있을 수 있다는 점도 문헌에 따라 언급된 바 있다. 본문에서는 계절 변동이 크지 않은 경우 가법형 계절 모형을 적용하는 것이 일반적이라는 내용으로 보완하였다.
ARIMA 모형. 자기회귀 누적이동평균 모형으로 알려진 ARIMA는 Box와 Jenkins(1970)의 연구에서 체계적으로 정립된 통계적 시계열 예측 기법이다. ARIMA는 시계열의 자기회귀(auto-regression)와 이동평균(moving average) 성분을 통합하고, 차분(differencing)을 통해 비정상성을 제거하는 통합(integration) 과정을 포함한다. 이를 통해 ARIMA 모형은 $(p,d,q)$ 차수의 모수로 시계열의 자기상관 구조를 표현할 수 있다. Box와 Jenkins의 원저서는 ARIMA 모형의 식별, 추정, 검증에 관한 체계적인 절차를 제시하여 Box-Jenkins 방법론으로 널리 알려졌으며, 이후 개정판에서도 계절형 ARIMA 및 예측 오차 분석 등의 내용이 추가되었다. 본 연구에서 언급한 ARIMA 모형의 모수 선정과 모형 적합 과정은 해당 원문과 일치하며, 특히 Box 등(2015)의 최신 개정판에 근거하여 보다 현대적인 해석을 반영하였다. 다만 기존 본문에서 참고한 일부 내용 중 ARIMA 모형의 모수 추정 방식에 관한 표현이 모호하였으므로, 최우추정법(MLE)을 활용한 모수 추정과 진단 과정에 대한 설명을 보완하였다.
Prophet 모형. Prophet은 Facebook의 데이터 과학자 Taylor와 Letham(2018)이 개발한 오픈 소스 시계열 예측 모형으로, 비전문가도 손쉽게 사용할 수 있도록 설계된 것이 특징이다. Prophet 모형은 추세(trend), 계절 효과(seasonality), 휴일 효과(holidays)의 세 가지 요소를 가법적 형태(additive model)로 결합하여 시간의 함수로 표현한다. 추세 함수는 일정 기간마다 기울기 변화를 허용하는 조각별 선형 또는 로그리스틱 성장 모델을 사용하고, 연간/주기적 계절성은 Fourier 항으로 구성된 주기 함수로 모델링되며, 휴일 효과는 사전 지정된 이벤트에 대한 영향으로 추가된다. Taylor와 Letham의 논문에 따르면 Prophet은 변화점(changepoint) 탐지를 통해 추세의 구조적 변화를 자동으로 포착하며, 베이지안 사전분포를 활용하여 과적합을 방지한다. 본 논문에서 기술한 Prophet 모형의 구성요소와 동작 원리는 원문과 부합하며, 특히 시계열의 추세와 계절성을 분리하고 예측하는 접근법으로서 ARIMA 등 전통 기법과 달리 비정상 시계열에도 직접 적용 가능한 점을 명확히 하였다. 인용에 있어서는 초기 공개된 arXiv 사전판(2017년)보다 학술지에 게재된 최종판(2018년)을 참고문헌으로 사용하는 것이 타당하므로, 해당 American Statistician 논문의 서지 정보를 제시하였다.
LSTM (Long Short-Term Memory). 장단기 메모리 신경망인 LSTM은 Hochreiter와 Schmidhuber(1997)가 제안한 순환신경망(RNN) 구조로, 기존 RNN의 장기가중치 소실 문제(vanishing gradient)를 해결하기 위해 고안되었다. LSTM은 셀 상태(cell state)를 통해 장기 의존 정보를 보존하고 입력게이트, 출력게이트, 망각게이트 등의 게이트 메커니즘을 도입함으로써 필요에 따라 정보들을 저장하거나 삭제할 수 있게 설계되었다. 원 논문에서 저자들은 LSTM이 매우 긴 시계열 의존 관계도 학습할 수 있음을 실험적으로 보여주었는데, 이는 본문에서 언급된 “기존 RNN으로는 학습하기 어려운 장기간의 패턴을 LSTM이 효과적으로 학습한다”는 내용과 일치한다. 다만 본문 중 LSTM의 동작에 대해 “은닉층을 여러 개 쌓은 심층 신경망”이라는 표현이 있었는데, 실제로 LSTM은 한 층의 내부에 게이트 구조를 갖는 단일 순환층으로 작동할 수도 있고, 복수 층으로 쌓아 딥 LSTM으로 확장할 수도 있다. 따라서 LSTM 자체는 단층 RNN 셀 구조를 지칭하는 것이며, 필요에 따라 다층으로 구성될 수 있다는 점을 원문에 근거하여 명확히 하였다. 또한 Hochreiter & Schmidhuber의 논문은 Neural Computation 저널에 게재되었으므로, 참고문헌에 해당 저널명과 논문 정보를 정확하게 기재하였다.
Transformer 모델. Transformer는 딥러닝 분야에서 혁신적인 시계열(순차) 처리 모형으로, Vaswani 등(2017)의 논문 “Attention Is All You Need”에서 처음 소개되었다. 이 모델은 인공신경망 기반 기법으로, 셀프 어텐션(self-attention) 메커니즘을 통해 시계열 데이터 내 모든 위치 간의 의미론적 관계를 한 번에 고려할 수 있게 하였다. Transformer는 인코더-디코더 구조를 가지며, 각 계층에서 다중 헤드 어텐션(multi-head attention)과 포지션-와이즈 피드포워드 신경망(position-wise feed-forward network)으로 구성된다. 순환구조(RNN)나 순차 합성곱(CNN)을 사용하지 않고도 병렬 처리 효율을 크게 높였다는 점에서 이전의 Seq2Seq 모델들과 구별된다. Vaswani 등의 원문에서는 이 모델이 기계번역 등 언어 처리 작업에서 당대 최고 수준의 성능을 달성함을 보였으며, 특히 어텐션 매트릭스를 통해 문장 내 단어 간의 관계를 시각적으로 해석할 수 있음을 강조한다. 본문에서는 Transformer를 설명하며 “인코더와 디코더에서 어텐션만으로 작동하는 구조”라고 기술하였는데, 이는 원 논문의 내용과 부합하는 표현이다. 다만 세부 사항 중 일부 부정확했던 부분을 바로잡았는데, 예컨대 본문 초기 버전에서 “Transformer가 8개의 인코더-디코더 블록으로 구성된다”는 식으로 고정된 구성처럼 서술된 것을 수정하였다. 실제로 Vaswani 등의 논문에서는 인코더와 디코더를 각각 6층씩 사용한 실험을 보고하였고, 어텐션 헤드도 8개를 사용한 모델을 제시하였으나, 이는 모델의 구성 예시일 뿐 계층 수나 헤드 수는 과업과 모델 설계에 따라 조정 가능하다. 따라서 Transformer의 일반적인 개념과 원리 위주로 설명하고, 특정 구현 세부치는 원문에 따라 필요한 경우에만 언급하도록 바로잡았다.
